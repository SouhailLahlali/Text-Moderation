{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jApVTFwXf7fd",
        "outputId": "d54a0742-5062-4bda-f1ec-16189d8658d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn8L_10ee3ST",
        "outputId": "43013198-0c84-4f63-fb65-7671ec19f911"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text: i'm gonna kill u motherfucker\n",
            "Classification: NSFW\n",
            "\n",
            "toxic          : 0.9961 --> Yes\n",
            "severe_toxic   : 0.6383 --> Yes\n",
            "obscene        : 0.9787 --> Yes\n",
            "threat         : 0.9109 --> Yes\n",
            "insult         : 0.8014 --> Yes\n",
            "identity_hate  : 0.0320 --> No\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Path to ur model\n",
        "file_path = ''\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(file_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(file_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def predict_toxicity(text, model, tokenizer, device, threshold=0.5, max_length=128):\n",
        "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits)  # Multi-label: use sigmoid\n",
        "\n",
        "    probs = probs.cpu().numpy()[0]\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    # Determine SFW/NSFW\n",
        "    nsfw_flag = \"NSFW\" if preds.sum() > 0 else \"SFW\"\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Classification: {nsfw_flag}\\n\")\n",
        "\n",
        "    for label, prob, pred in zip(label_cols, probs, preds):\n",
        "        print(f\"{label:15}: {prob:.4f} --> {'Yes' if pred else 'No'}\")\n",
        "\n",
        "    return nsfw_flag, preds, probs\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Put ur text here\n",
        "    text = \"\"\n",
        "\n",
        "    nsfw_flag, preds, probs = predict_toxicity(text, model, tokenizer, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEtjePosglLB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
